{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data processing\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from nptdms import TdmsFile #Process ramping file\n",
    "\n",
    "#For building ML models\n",
    "import keras\n",
    "import keras.models as models\n",
    "from keras.layers.core import Dense\n",
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_channel_and_time(dir_path, channel):\n",
    "    data_frame = pd.DataFrame(data = {channel: np.load(dir_path + channel + \".npy\"),\n",
    "                                     \"time\": np.load(dir_path + \"time.npy\")})\n",
    "    return data_frame\n",
    "    \n",
    "def generate_data_0(dir_path, channel, time_range, window = 1000, step = 10):\n",
    "    #Load the data\n",
    "    data = load_channel_and_time(dir_path, channel)\n",
    "    \n",
    "    #Select the part\n",
    "    start = time_range[0]\n",
    "    end = time_range[1]\n",
    "    data = data[(data[\"time\"] > start) & (data[\"time\"] < end)]\n",
    "    \n",
    "    #Calculate the statistics\n",
    "    #data[\"Mean\"] = data.loc[:, channel].abs().rolling(window=window).mean()\n",
    "    data[\"SD\"] = data.loc[:, channel].rolling(window=window).std()\n",
    "    #data[\"Kurtosis\"] = data.loc[:, channel].rolling(window=window).kurt()\n",
    "    #data[\"Skew\"] = data.loc[:, channel].rolling(window=window).skew()\n",
    "    \n",
    "    select_list = [\"SD\"]\n",
    "    \n",
    "    assert data[select_list].to_numpy()[window-1::step].shape[0] == data['time'].to_numpy()[window-1::step].shape[0]\n",
    "    \n",
    "    %reset -f in\n",
    "    \n",
    "    return data[select_list].to_numpy()[window-1::step], data['time'].to_numpy()[window-1::step]\n",
    "\n",
    "def generate_data_no_time(dir_path, channel, time_range, window = 1000, step = 10):\n",
    "    #Load the data\n",
    "    data = load_channel_and_time(dir_path, channel)\n",
    "    \n",
    "    #Select the part\n",
    "    start = time_range[0]\n",
    "    end = time_range[1]\n",
    "    data = data[(data[\"time\"] > start) & (data[\"time\"] < end)]\n",
    "    \n",
    "    #Calculate the statistics\n",
    "    #data[\"Mean\"] = data.loc[:, channel].abs().rolling(window=window).mean()\n",
    "    data[\"SD\"] = data.loc[:, channel].rolling(window=window).std()\n",
    "    #data[\"Kurtosis\"] = data.loc[:, channel].rolling(window=window).kurt()\n",
    "    #data[\"Skew\"] = data.loc[:, channel].rolling(window=window).skew()\n",
    "    \n",
    "    select_list = [\"SD\"]\n",
    "    \n",
    "    %reset -f in\n",
    "    \n",
    "    return data[select_list].to_numpy()[window-1::step]\n",
    "\n",
    "def generate_data_all_sensors(dir_path, time_range, window = 1000, step = 10):\n",
    "    \n",
    "    ai0, time = generate_data(dir_path, \"ai0\", time_range = time_range, window = window, step = step)\n",
    "    ai1 = generate_data_no_time(dir_path, \"ai1\", time_range = time_range, window = window, step = step)\n",
    "    ai2 = generate_data_no_time(dir_path, \"ai2\", time_range = time_range, window = window, step = step)\n",
    "    ai3 = generate_data_no_time(dir_path, \"ai3\", time_range = time_range, window = window, step = step)\n",
    "    ai4 = generate_data_no_time(dir_path, \"ai4\", time_range = time_range, window = window, step = step)\n",
    "    \n",
    "    #Multiply them all together\n",
    "    product_var = ai0*ai1*ai2*ai3*ai4\n",
    "    \n",
    "    \n",
    "    all_channels = np.concatenate((ai0,ai1,ai2,ai3,ai4,product_var), axis = 1)\n",
    "    \n",
    "    %reset -f in\n",
    "    \n",
    "    return all_channels, time\n",
    "\n",
    "def plot_moving_mean(dir_path, channel, time_range, window = 1000, step = 10):\n",
    "    #Load the data\n",
    "    data = load_channel_and_time(dir_path, channel)\n",
    "    \n",
    "    #Select the part\n",
    "    start = time_range[0]\n",
    "    end = time_range[1]\n",
    "    data = data[(data[\"time\"] > start) & (data[\"time\"] < end)]\n",
    "    \n",
    "    #Calculate the mean\n",
    "    data[\"Mean\"] = data.loc[:, channel].abs().rolling(window=window).mean()\n",
    "    \n",
    "    #Plot\n",
    "    plt.figure(figsize=(20,2))\n",
    "    plt.plot(data['time'].to_numpy()[window-1::step], data[\"Mean\"].to_numpy()[window-1::step], label = \"Mean of abs(data)\")\n",
    "    plt.legend(loc = \"upper right\")\n",
    "    plt.xlabel(\"Time (s)\")\n",
    "    plt.title(\"Sensor {}'s moving mean\".format(channel))\n",
    "    \n",
    "    %reset -f in\n",
    "    \n",
    "def load_sensor(dir_path, sensor, time_range = None):\n",
    "    \n",
    "    data = pd.DataFrame(data = {sensor: np.load(dir_path + sensor + \".npy\"),\n",
    "                                \"time\": np.load(dir_path + \"time.npy\")})\n",
    "    \n",
    "    start = min(data[\"time\"])\n",
    "    end = max(data[\"time\"])\n",
    "    \n",
    "    if time_range:\n",
    "        start = time_range[0]\n",
    "        end = time_range[1]\n",
    "    \n",
    "    \n",
    "    data = data[(data[\"time\"] > start) & (data[\"time\"] < end)]\n",
    "   \n",
    "    %reset -f in\n",
    "    \n",
    "    return data\n",
    "    \n",
    "def plot_product_mean(dir_path, time_range, window = 1000, step = 10):\n",
    "    #Load the data\n",
    "    ai0 = load_sensor(dir_path, \"ai0\", time_range = time_range)\n",
    "    ai1 = load_sensor(dir_path, \"ai1\", time_range = time_range)[\"ai1\"].abs().rolling(window=window).mean().to_numpy()[window-1::step]\n",
    "    ai2 = load_sensor(dir_path, \"ai2\", time_range = time_range)[\"ai2\"].abs().rolling(window=window).mean().to_numpy()[window-1::step]\n",
    "    ai3 = load_sensor(dir_path, \"ai3\", time_range = time_range)[\"ai3\"].abs().rolling(window=window).mean().to_numpy()[window-1::step]\n",
    "    ai4 = load_sensor(dir_path, \"ai4\", time_range = time_range)[\"ai4\"].abs().rolling(window=window).mean().to_numpy()[window-1::step]\n",
    "    \n",
    "    time_axis = ai0['time'].to_numpy()[window-1::step]\n",
    "    \n",
    "    ai0 = ai0[\"ai0\"].abs().rolling(window=window).mean().to_numpy()[window-1::step]\n",
    "    \n",
    "    product = ai0*ai1*ai2*ai3*ai4\n",
    "    \n",
    "    #Plot\n",
    "    plt.figure(figsize=(20,2))\n",
    "    plt.plot(time_axis, product)\n",
    "    plt.xlabel(\"Time (s)\")\n",
    "    plt.title(\"Product of moving means\")\n",
    "    \n",
    "def plot_SD(dir_path, channel, time_range, window = 2000, step = 10):\n",
    "    \n",
    "    #Load the data\n",
    "    data = load_channel_and_time(dir_path, channel)\n",
    "    \n",
    "    #Select the part\n",
    "    start = time_range[0]\n",
    "    end = time_range[1]\n",
    "    data = data[(data[\"time\"] > start) & (data[\"time\"] < end)]\n",
    "    \n",
    "    #Calculate the mean\n",
    "    data[\"SD\"] = data.loc[:, channel].abs().rolling(window=window).std()\n",
    "    \n",
    "    #Plot\n",
    "    plt.figure(figsize=(20,2))\n",
    "    plt.plot(data['time'].to_numpy()[window-1::step], data[\"SD\"].to_numpy()[window-1::step], label = \"Standard deviation\", color = \"red\")\n",
    "    plt.legend(loc = \"upper right\")\n",
    "    plt.xlabel(\"Time (s)\")\n",
    "    plt.title(\"Sensor {}'s moving standard deviation\".format(channel))\n",
    "    \n",
    "    %reset -f in\n",
    "\n",
    "\n",
    "def plot_product_SD(dir_path, time_range, window = 1000, step = 10):\n",
    "    #Load the data\n",
    "    ai0 = load_sensor(dir_path, \"ai0\", time_range = time_range)\n",
    "    ai1 = load_sensor(dir_path, \"ai1\", time_range = time_range)[\"ai1\"].rolling(window=window).std().to_numpy()[window-1::step]\n",
    "    ai2 = load_sensor(dir_path, \"ai2\", time_range = time_range)[\"ai2\"].rolling(window=window).std().to_numpy()[window-1::step]\n",
    "    ai3 = load_sensor(dir_path, \"ai3\", time_range = time_range)[\"ai3\"].rolling(window=window).std().to_numpy()[window-1::step]\n",
    "    ai4 = load_sensor(dir_path, \"ai4\", time_range = time_range)[\"ai4\"].rolling(window=window).std().to_numpy()[window-1::step]\n",
    "    \n",
    "    time_axis = ai0['time'].to_numpy()[window-1::step]\n",
    "    \n",
    "    ai0 = ai0[\"ai0\"].rolling(window=window).std().to_numpy()[window-1::step]\n",
    "    \n",
    "    product = ai0*ai1*ai2*ai3*ai4\n",
    "    \n",
    "    #Plot\n",
    "    plt.figure(figsize=(20,2))\n",
    "    plt.plot(time_axis, product, color = \"red\")\n",
    "    plt.xlabel(\"Time (s)\")\n",
    "    plt.title(\"Product of moving standard deviations\")\n",
    "    \n",
    "def generate_mean_data(dir_path, time_range, window = 2000, step = 10):\n",
    "    #Load the data\n",
    "    ai0 = load_sensor(dir_path, \"ai0\", time_range = time_range)[\"ai0\"].abs().rolling(window=window).mean().to_numpy()[window-1::step]\n",
    "    ai1 = load_sensor(dir_path, \"ai1\", time_range = time_range)[\"ai1\"].abs().rolling(window=window).mean().to_numpy()[window-1::step]\n",
    "    ai2 = load_sensor(dir_path, \"ai2\", time_range = time_range)[\"ai2\"].abs().rolling(window=window).mean().to_numpy()[window-1::step]\n",
    "    ai3 = load_sensor(dir_path, \"ai3\", time_range = time_range)[\"ai3\"].abs().rolling(window=window).mean().to_numpy()[window-1::step]\n",
    "    ai4 = load_sensor(dir_path, \"ai4\", time_range = time_range)[\"ai4\"].abs().rolling(window=window).mean().to_numpy()[window-1::step]\n",
    "    \n",
    "    #Calculate the product\n",
    "    product = ai0*ai1*ai2*ai3*ai4\n",
    "    \n",
    "    #Stack them together\n",
    "    all_mean = np.vstack((ai0,ai1,ai2,ai3,ai4, product)).transpose()\n",
    "    \n",
    "    %reset -f in\n",
    "    \n",
    "    return all_mean\n",
    "\n",
    "def generate_sd_data(dir_path, time_range, window = 2000, step = 10):\n",
    "    #Load the data\n",
    "    ai0 = load_sensor(dir_path, \"ai0\", time_range = time_range)[\"ai0\"].rolling(window=window).std().to_numpy()[window-1::step]\n",
    "    ai1 = load_sensor(dir_path, \"ai1\", time_range = time_range)[\"ai1\"].rolling(window=window).std().to_numpy()[window-1::step]\n",
    "    ai2 = load_sensor(dir_path, \"ai2\", time_range = time_range)[\"ai2\"].rolling(window=window).std().to_numpy()[window-1::step]\n",
    "    ai3 = load_sensor(dir_path, \"ai3\", time_range = time_range)[\"ai3\"].rolling(window=window).std().to_numpy()[window-1::step]\n",
    "    ai4 = load_sensor(dir_path, \"ai4\", time_range = time_range)[\"ai4\"].rolling(window=window).std().to_numpy()[window-1::step]\n",
    "    \n",
    "    #Calculate the product\n",
    "    product = ai0*ai1*ai2*ai3*ai4\n",
    "    \n",
    "    #Stack them together\n",
    "    all_sd = np.vstack((ai0,ai1,ai2,ai3,ai4, product)).transpose()\n",
    "    \n",
    "    %reset -f in\n",
    "    \n",
    "    return all_sd\n",
    "\n",
    "def load_time_label(dir_path, time_range, window = 2000, step = 10):\n",
    "    \n",
    "    time_label =  np.load(dir_path + \"time.npy\")\n",
    "    \n",
    "    start = min(time_label)\n",
    "    end = max(time_label)\n",
    "    \n",
    "    if time_range:\n",
    "        start = time_range[0]\n",
    "        end = time_range[1]\n",
    "    \n",
    "    \n",
    "    time_label = time_label[(time_label > start) & (time_label < end)][window-1::step]\n",
    "    \n",
    "    %reset -f in\n",
    "    \n",
    "    return time_label\n",
    "    \n",
    "\n",
    "def generate_data(dir_path, time_range, window = 2000, step = 10):\n",
    "    \n",
    "    moving_mean = generate_mean_data(dir_path, time_range, window = 2000, step = 10)\n",
    "    moving_sd = generate_sd_data(dir_path, time_range, window = 2000, step = 10)\n",
    "    time_label = load_time_label(dir_path, time_range, window = 2000, step = 10)\n",
    "    \n",
    "    all_data = np.concatenate((moving_mean, moving_sd), axis = 1)\n",
    "    \n",
    "    %reset -f in\n",
    "    \n",
    "    return all_data, time_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flushing input history\n",
      "Flushing input history\n",
      "Flushing input history\n",
      "Flushing input history\n",
      "Flushing input history\n",
      "Flushing input history\n",
      "Flushing input history\n",
      "Flushing input history\n",
      "Flushing input history\n",
      "Flushing input history\n",
      "Flushing input history\n",
      "Flushing input history\n",
      "Flushing input history\n",
      "Flushing input history\n"
     ]
    }
   ],
   "source": [
    "X_train, train_time = generate_data(\"./data/Ramp21/\", time_range = (-450, -420), window = 2000, step = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train = scaler.transform(X_train)\n",
    "\n",
    "#X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Keras version:  2.3.1\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 6)                 78        \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 21        \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 6)                 24        \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 12)                84        \n",
      "=================================================================\n",
      "Total params: 207\n",
      "Trainable params: 207\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#================BUILD THE MODEL====================\n",
    "print(\"Using Keras version: \", keras.__version__)\n",
    "\n",
    "# Simple model\n",
    "model = models.Sequential()\n",
    "\n",
    "model.add(Dense(6, activation = 'elu', kernel_initializer='glorot_uniform',\n",
    "                kernel_regularizer=regularizers.l2(0.0),\n",
    "                input_dim=X_train.shape[1]))\n",
    "\n",
    "model.add(Dense(3,activation='elu',\n",
    "                kernel_initializer='glorot_uniform'))\n",
    "\n",
    "model.add(Dense(6,activation='elu',\n",
    "                kernel_initializer='glorot_uniform'))\n",
    "\n",
    "model.add(Dense(X_train.shape[1],\n",
    "                kernel_initializer='glorot_uniform'))\n",
    "\n",
    "model.compile(loss='mse',optimizer='adam')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 269820 samples, validate on 29980 samples\n",
      "Epoch 1/20\n",
      "269820/269820 [==============================] - 3s 12us/step - loss: 0.0022 - val_loss: 0.0025\n",
      "Epoch 2/20\n",
      "269820/269820 [==============================] - 2s 7us/step - loss: 6.3256e-04 - val_loss: 0.0016\n",
      "Epoch 3/20\n",
      "269820/269820 [==============================] - 2s 6us/step - loss: 3.6246e-04 - val_loss: 0.0014\n",
      "Epoch 4/20\n",
      "269820/269820 [==============================] - 2s 6us/step - loss: 2.5459e-04 - val_loss: 0.0014\n",
      "Epoch 5/20\n",
      "269820/269820 [==============================] - 2s 7us/step - loss: 2.2022e-04 - val_loss: 0.0013\n",
      "Epoch 6/20\n",
      "269820/269820 [==============================] - 2s 6us/step - loss: 2.0654e-04 - val_loss: 0.0012\n",
      "Epoch 7/20\n",
      "269820/269820 [==============================] - 2s 6us/step - loss: 1.9849e-04 - val_loss: 0.0011\n",
      "Epoch 8/20\n",
      "269820/269820 [==============================] - 2s 6us/step - loss: 1.9211e-04 - val_loss: 0.0011\n",
      "Epoch 9/20\n",
      "269820/269820 [==============================] - 2s 7us/step - loss: 1.8667e-04 - val_loss: 0.0010\n",
      "Epoch 10/20\n",
      "269820/269820 [==============================] - 2s 7us/step - loss: 1.8132e-04 - val_loss: 0.0010\n",
      "Epoch 11/20\n",
      "269820/269820 [==============================] - 2s 6us/step - loss: 1.7597e-04 - val_loss: 9.6274e-04\n",
      "Epoch 12/20\n",
      "269820/269820 [==============================] - 2s 6us/step - loss: 1.7092e-04 - val_loss: 9.6974e-04\n",
      "Epoch 13/20\n",
      "269820/269820 [==============================] - 2s 7us/step - loss: 1.6641e-04 - val_loss: 9.2953e-04\n",
      "Epoch 14/20\n",
      "269820/269820 [==============================] - 2s 6us/step - loss: 1.6237e-04 - val_loss: 9.1184e-04\n",
      "Epoch 15/20\n",
      "269820/269820 [==============================] - 2s 6us/step - loss: 1.5900e-04 - val_loss: 8.9907e-04\n",
      "Epoch 16/20\n",
      "269820/269820 [==============================] - 2s 6us/step - loss: 1.5635e-04 - val_loss: 8.6560e-04\n",
      "Epoch 17/20\n",
      "269820/269820 [==============================] - 2s 6us/step - loss: 1.5426e-04 - val_loss: 8.8353e-04\n",
      "Epoch 18/20\n",
      "269820/269820 [==============================] - 2s 6us/step - loss: 1.5246e-04 - val_loss: 8.6839e-04\n",
      "Epoch 19/20\n",
      "269820/269820 [==============================] - 2s 6us/step - loss: 1.5098e-04 - val_loss: 8.5345e-04\n",
      "Epoch 20/20\n",
      "269820/269820 [==============================] - 2s 6us/step - loss: 1.4986e-04 - val_loss: 8.5598e-04\n"
     ]
    }
   ],
   "source": [
    "# Train model for 100 epochs, batch size of 10: \n",
    "NUM_EPOCHS=20\n",
    "BATCH_SIZE=1028\n",
    "\n",
    "history=model.fit(X_train, X_train,\n",
    "                  batch_size=BATCH_SIZE, \n",
    "                  epochs=NUM_EPOCHS,\n",
    "                  validation_split=0.1,\n",
    "                  verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.save(\"models/Ramp21-12in.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/uscms_data/d3/dhoang/quench-detector\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
