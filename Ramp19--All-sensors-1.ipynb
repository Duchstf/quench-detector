{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ramp 19 -- Mean + Variances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Bad key \"text.kerning_factor\" on line 4 in\n",
      "/uscms_data/d3/dhoang/miniconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test_patch.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.1.3/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#Data processing\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from nptdms import TdmsFile #Process ramping file\n",
    "\n",
    "#For building ML models\n",
    "import keras\n",
    "import keras.models as models\n",
    "from keras.layers.core import Dense\n",
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_channel_and_time(dir_path, channel):\n",
    "    data_frame = pd.DataFrame(data = {channel: np.load(dir_path + channel + \".npy\"),\n",
    "                                     \"time\": np.load(dir_path + \"time.npy\")})\n",
    "    return data_frame\n",
    "    \n",
    "def generate_data_0(dir_path, channel, time_range, window = 1000, step = 10):\n",
    "    #Load the data\n",
    "    data = load_channel_and_time(dir_path, channel)\n",
    "    \n",
    "    #Select the part\n",
    "    start = time_range[0]\n",
    "    end = time_range[1]\n",
    "    data = data[(data[\"time\"] > start) & (data[\"time\"] < end)]\n",
    "    \n",
    "    #Calculate the statistics\n",
    "    #data[\"Mean\"] = data.loc[:, channel].abs().rolling(window=window).mean()\n",
    "    data[\"SD\"] = data.loc[:, channel].rolling(window=window).std()\n",
    "    #data[\"Kurtosis\"] = data.loc[:, channel].rolling(window=window).kurt()\n",
    "    #data[\"Skew\"] = data.loc[:, channel].rolling(window=window).skew()\n",
    "    \n",
    "    select_list = [\"SD\"]\n",
    "    \n",
    "    assert data[select_list].to_numpy()[window-1::step].shape[0] == data['time'].to_numpy()[window-1::step].shape[0]\n",
    "    \n",
    "    %reset -f in\n",
    "    \n",
    "    return data[select_list].to_numpy()[window-1::step], data['time'].to_numpy()[window-1::step]\n",
    "\n",
    "def generate_data_no_time(dir_path, channel, time_range, window = 1000, step = 10):\n",
    "    #Load the data\n",
    "    data = load_channel_and_time(dir_path, channel)\n",
    "    \n",
    "    #Select the part\n",
    "    start = time_range[0]\n",
    "    end = time_range[1]\n",
    "    data = data[(data[\"time\"] > start) & (data[\"time\"] < end)]\n",
    "    \n",
    "    #Calculate the statistics\n",
    "    #data[\"Mean\"] = data.loc[:, channel].abs().rolling(window=window).mean()\n",
    "    data[\"SD\"] = data.loc[:, channel].rolling(window=window).std()\n",
    "    #data[\"Kurtosis\"] = data.loc[:, channel].rolling(window=window).kurt()\n",
    "    #data[\"Skew\"] = data.loc[:, channel].rolling(window=window).skew()\n",
    "    \n",
    "    select_list = [\"SD\"]\n",
    "    \n",
    "    %reset -f in\n",
    "    \n",
    "    return data[select_list].to_numpy()[window-1::step]\n",
    "\n",
    "def generate_data_all_sensors(dir_path, time_range, window = 1000, step = 10):\n",
    "    \n",
    "    ai0, time = generate_data(dir_path, \"ai0\", time_range = time_range, window = window, step = step)\n",
    "    ai1 = generate_data_no_time(dir_path, \"ai1\", time_range = time_range, window = window, step = step)\n",
    "    ai2 = generate_data_no_time(dir_path, \"ai2\", time_range = time_range, window = window, step = step)\n",
    "    ai3 = generate_data_no_time(dir_path, \"ai3\", time_range = time_range, window = window, step = step)\n",
    "    ai4 = generate_data_no_time(dir_path, \"ai4\", time_range = time_range, window = window, step = step)\n",
    "    \n",
    "    #Multiply them all together\n",
    "    product_var = ai0*ai1*ai2*ai3*ai4\n",
    "    \n",
    "    \n",
    "    all_channels = np.concatenate((ai0,ai1,ai2,ai3,ai4,product_var), axis = 1)\n",
    "    \n",
    "    %reset -f in\n",
    "    \n",
    "    return all_channels, time\n",
    "\n",
    "def plot_moving_mean(dir_path, channel, time_range, window = 1000, step = 10):\n",
    "    #Load the data\n",
    "    data = load_channel_and_time(dir_path, channel)\n",
    "    \n",
    "    #Select the part\n",
    "    start = time_range[0]\n",
    "    end = time_range[1]\n",
    "    data = data[(data[\"time\"] > start) & (data[\"time\"] < end)]\n",
    "    \n",
    "    #Calculate the mean\n",
    "    data[\"Mean\"] = data.loc[:, channel].abs().rolling(window=window).mean()\n",
    "    \n",
    "    #Plot\n",
    "    plt.figure(figsize=(20,2))\n",
    "    plt.plot(data['time'].to_numpy()[window-1::step], data[\"Mean\"].to_numpy()[window-1::step], label = \"Mean of abs(data)\")\n",
    "    plt.legend(loc = \"upper right\")\n",
    "    plt.xlabel(\"Time (s)\")\n",
    "    plt.title(\"Sensor {}'s moving mean\".format(channel))\n",
    "    \n",
    "    %reset -f in\n",
    "    \n",
    "def load_sensor(dir_path, sensor, time_range = None):\n",
    "    \n",
    "    data = pd.DataFrame(data = {sensor: np.load(dir_path + sensor + \".npy\"),\n",
    "                                \"time\": np.load(dir_path + \"time.npy\")})\n",
    "    \n",
    "    start = min(data[\"time\"])\n",
    "    end = max(data[\"time\"])\n",
    "    \n",
    "    if time_range:\n",
    "        start = time_range[0]\n",
    "        end = time_range[1]\n",
    "    \n",
    "    \n",
    "    data = data[(data[\"time\"] > start) & (data[\"time\"] < end)]\n",
    "   \n",
    "    %reset -f in\n",
    "    \n",
    "    return data\n",
    "    \n",
    "def plot_product_mean(dir_path, time_range, window = 1000, step = 10):\n",
    "    #Load the data\n",
    "    ai0 = load_sensor(dir_path, \"ai0\", time_range = time_range)\n",
    "    ai1 = load_sensor(dir_path, \"ai1\", time_range = time_range)[\"ai1\"].abs().rolling(window=window).mean().to_numpy()[window-1::step]\n",
    "    ai2 = load_sensor(dir_path, \"ai2\", time_range = time_range)[\"ai2\"].abs().rolling(window=window).mean().to_numpy()[window-1::step]\n",
    "    ai3 = load_sensor(dir_path, \"ai3\", time_range = time_range)[\"ai3\"].abs().rolling(window=window).mean().to_numpy()[window-1::step]\n",
    "    ai4 = load_sensor(dir_path, \"ai4\", time_range = time_range)[\"ai4\"].abs().rolling(window=window).mean().to_numpy()[window-1::step]\n",
    "    \n",
    "    time_axis = ai0['time'].to_numpy()[window-1::step]\n",
    "    \n",
    "    ai0 = ai0[\"ai0\"].abs().rolling(window=window).mean().to_numpy()[window-1::step]\n",
    "    \n",
    "    product = ai0*ai1*ai2*ai3*ai4\n",
    "    \n",
    "    #Plot\n",
    "    plt.figure(figsize=(20,2))\n",
    "    plt.plot(time_axis, product)\n",
    "    plt.xlabel(\"Time (s)\")\n",
    "    plt.title(\"Product of moving means\")\n",
    "    \n",
    "def plot_SD(dir_path, channel, time_range, window = 2000, step = 10):\n",
    "    \n",
    "    #Load the data\n",
    "    data = load_channel_and_time(dir_path, channel)\n",
    "    \n",
    "    #Select the part\n",
    "    start = time_range[0]\n",
    "    end = time_range[1]\n",
    "    data = data[(data[\"time\"] > start) & (data[\"time\"] < end)]\n",
    "    \n",
    "    #Calculate the mean\n",
    "    data[\"SD\"] = data.loc[:, channel].abs().rolling(window=window).std()\n",
    "    \n",
    "    #Plot\n",
    "    plt.figure(figsize=(20,2))\n",
    "    plt.plot(data['time'].to_numpy()[window-1::step], data[\"SD\"].to_numpy()[window-1::step], label = \"Standard deviation\", color = \"red\")\n",
    "    plt.legend(loc = \"upper right\")\n",
    "    plt.xlabel(\"Time (s)\")\n",
    "    plt.title(\"Sensor {}'s moving standard deviation\".format(channel))\n",
    "    \n",
    "    %reset -f in\n",
    "\n",
    "\n",
    "def plot_product_SD(dir_path, time_range, window = 1000, step = 10):\n",
    "    #Load the data\n",
    "    ai0 = load_sensor(dir_path, \"ai0\", time_range = time_range)\n",
    "    ai1 = load_sensor(dir_path, \"ai1\", time_range = time_range)[\"ai1\"].rolling(window=window).std().to_numpy()[window-1::step]\n",
    "    ai2 = load_sensor(dir_path, \"ai2\", time_range = time_range)[\"ai2\"].rolling(window=window).std().to_numpy()[window-1::step]\n",
    "    ai3 = load_sensor(dir_path, \"ai3\", time_range = time_range)[\"ai3\"].rolling(window=window).std().to_numpy()[window-1::step]\n",
    "    ai4 = load_sensor(dir_path, \"ai4\", time_range = time_range)[\"ai4\"].rolling(window=window).std().to_numpy()[window-1::step]\n",
    "    \n",
    "    time_axis = ai0['time'].to_numpy()[window-1::step]\n",
    "    \n",
    "    ai0 = ai0[\"ai0\"].rolling(window=window).std().to_numpy()[window-1::step]\n",
    "    \n",
    "    product = ai0*ai1*ai2*ai3*ai4\n",
    "    \n",
    "    #Plot\n",
    "    plt.figure(figsize=(20,2))\n",
    "    plt.plot(time_axis, product, color = \"red\")\n",
    "    plt.xlabel(\"Time (s)\")\n",
    "    plt.title(\"Product of moving standard deviations\")\n",
    "    \n",
    "def generate_mean_data(dir_path, time_range, window = 2000, step = 10):\n",
    "    #Load the data\n",
    "    ai0 = load_sensor(dir_path, \"ai0\", time_range = time_range)[\"ai0\"].abs().rolling(window=window).mean().to_numpy()[window-1::step]\n",
    "    ai1 = load_sensor(dir_path, \"ai1\", time_range = time_range)[\"ai1\"].abs().rolling(window=window).mean().to_numpy()[window-1::step]\n",
    "    ai2 = load_sensor(dir_path, \"ai2\", time_range = time_range)[\"ai2\"].abs().rolling(window=window).mean().to_numpy()[window-1::step]\n",
    "    ai3 = load_sensor(dir_path, \"ai3\", time_range = time_range)[\"ai3\"].abs().rolling(window=window).mean().to_numpy()[window-1::step]\n",
    "    ai4 = load_sensor(dir_path, \"ai4\", time_range = time_range)[\"ai4\"].abs().rolling(window=window).mean().to_numpy()[window-1::step]\n",
    "    \n",
    "    #Calculate the product\n",
    "    product = ai0*ai1*ai2*ai3*ai4\n",
    "    \n",
    "    #Stack them together\n",
    "    all_mean = np.vstack((ai0,ai1,ai2,ai3,ai4, product)).transpose()\n",
    "    \n",
    "    %reset -f in\n",
    "    \n",
    "    return all_mean\n",
    "\n",
    "def generate_sd_data(dir_path, time_range, window = 2000, step = 10):\n",
    "    #Load the data\n",
    "    ai0 = load_sensor(dir_path, \"ai0\", time_range = time_range)[\"ai0\"].rolling(window=window).std().to_numpy()[window-1::step]\n",
    "    ai1 = load_sensor(dir_path, \"ai1\", time_range = time_range)[\"ai1\"].rolling(window=window).std().to_numpy()[window-1::step]\n",
    "    ai2 = load_sensor(dir_path, \"ai2\", time_range = time_range)[\"ai2\"].rolling(window=window).std().to_numpy()[window-1::step]\n",
    "    ai3 = load_sensor(dir_path, \"ai3\", time_range = time_range)[\"ai3\"].rolling(window=window).std().to_numpy()[window-1::step]\n",
    "    ai4 = load_sensor(dir_path, \"ai4\", time_range = time_range)[\"ai4\"].rolling(window=window).std().to_numpy()[window-1::step]\n",
    "    \n",
    "    #Calculate the product\n",
    "    product = ai0*ai1*ai2*ai3*ai4\n",
    "    \n",
    "    #Stack them together\n",
    "    all_sd = np.vstack((ai0,ai1,ai2,ai3,ai4, product)).transpose()\n",
    "    \n",
    "    %reset -f in\n",
    "    \n",
    "    return all_sd\n",
    "\n",
    "def load_time_label(dir_path, time_range, window = 2000, step = 10):\n",
    "    \n",
    "    time_label =  np.load(dir_path + \"time.npy\")\n",
    "    \n",
    "    start = min(time_label)\n",
    "    end = max(time_label)\n",
    "    \n",
    "    if time_range:\n",
    "        start = time_range[0]\n",
    "        end = time_range[1]\n",
    "    \n",
    "    \n",
    "    time_label = time_label[(time_label > start) & (time_label < end)][window-1::step]\n",
    "    \n",
    "    %reset -f in\n",
    "    \n",
    "    return time_label\n",
    "    \n",
    "\n",
    "def generate_data(dir_path, time_range, window = 2000, step = 10):\n",
    "    \n",
    "    moving_mean = generate_mean_data(dir_path, time_range, window = 2000, step = 10)\n",
    "    moving_sd = generate_sd_data(dir_path, time_range, window = 2000, step = 10)\n",
    "    time_label = load_time_label(dir_path, time_range, window = 2000, step = 10)\n",
    "    \n",
    "    all_data = np.concatenate((moving_mean, moving_sd), axis = 1)\n",
    "    \n",
    "    %reset -f in\n",
    "    \n",
    "    return all_data, time_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Starting point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flushing input history\n",
      "Flushing input history\n",
      "Flushing input history\n",
      "Flushing input history\n",
      "Flushing input history\n",
      "Flushing input history\n",
      "Flushing input history\n",
      "Flushing input history\n",
      "Flushing input history\n",
      "Flushing input history\n",
      "Flushing input history\n",
      "Flushing input history\n",
      "Flushing input history\n",
      "Flushing input history\n",
      "Flushing input history\n",
      "Flushing input history\n",
      "Flushing input history\n",
      "Flushing input history\n",
      "Flushing input history\n",
      "Flushing input history\n",
      "Flushing input history\n",
      "Flushing input history\n",
      "Flushing input history\n",
      "Flushing input history\n",
      "Flushing input history\n",
      "Flushing input history\n",
      "Flushing input history\n",
      "Flushing input history\n",
      "X_train's shape:  (999800, 12)\n",
      "X_test's shape:  (999800, 12)\n"
     ]
    }
   ],
   "source": [
    "X_train, train_time = generate_data(\"./data/Ramp19/\", time_range = (-400, -300), window = 2000, step = 10)\n",
    "X_test, test_time = generate_data(\"./data/Ramp19/\", time_range = (-100, 0), window = 2000, step = 10)\n",
    "print(\"X_train's shape: \", X_train.shape)\n",
    "print(\"X_test's shape: \", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train = scaler.transform(X_train)\n",
    "\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Keras version:  2.3.1\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 6)                 78        \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 21        \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 6)                 24        \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 12)                84        \n",
      "=================================================================\n",
      "Total params: 207\n",
      "Trainable params: 207\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#================BUILD THE MODEL====================\n",
    "print(\"Using Keras version: \", keras.__version__)\n",
    "\n",
    "# Simple model\n",
    "model = models.Sequential()\n",
    "\n",
    "model.add(Dense(6, activation = 'elu', kernel_initializer='glorot_uniform',\n",
    "                kernel_regularizer=regularizers.l2(0.0),\n",
    "                input_dim=X_train.shape[1]))\n",
    "\n",
    "model.add(Dense(3,activation='elu',\n",
    "                kernel_initializer='glorot_uniform'))\n",
    "\n",
    "model.add(Dense(6,activation='elu',\n",
    "                kernel_initializer='glorot_uniform'))\n",
    "\n",
    "model.add(Dense(X_train.shape[1],\n",
    "                kernel_initializer='glorot_uniform'))\n",
    "\n",
    "model.compile(loss='mse',optimizer='adam')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 899820 samples, validate on 99980 samples\n",
      "Epoch 1/50\n",
      "899820/899820 [==============================] - 8s 9us/step - loss: 0.0014 - val_loss: 7.5700e-04\n",
      "Epoch 2/50\n",
      "899820/899820 [==============================] - 7s 8us/step - loss: 2.7270e-04 - val_loss: 3.8444e-04\n",
      "Epoch 3/50\n",
      "899820/899820 [==============================] - 8s 8us/step - loss: 2.0995e-04 - val_loss: 3.2029e-04\n",
      "Epoch 4/50\n",
      "899820/899820 [==============================] - 8s 9us/step - loss: 1.8614e-04 - val_loss: 2.8622e-04\n",
      "Epoch 5/50\n",
      "899820/899820 [==============================] - 7s 8us/step - loss: 1.6197e-04 - val_loss: 2.6092e-04\n",
      "Epoch 6/50\n",
      "899820/899820 [==============================] - 7s 8us/step - loss: 1.4775e-04 - val_loss: 2.5030e-04\n",
      "Epoch 7/50\n",
      "899820/899820 [==============================] - 7s 8us/step - loss: 1.4360e-04 - val_loss: 2.4771e-04\n",
      "Epoch 8/50\n",
      "899820/899820 [==============================] - 7s 8us/step - loss: 1.4085e-04 - val_loss: 2.3847e-04\n",
      "Epoch 9/50\n",
      "899820/899820 [==============================] - 7s 8us/step - loss: 1.3814e-04 - val_loss: 2.3946e-04\n",
      "Epoch 10/50\n",
      "899820/899820 [==============================] - 7s 8us/step - loss: 1.3539e-04 - val_loss: 2.3406e-04\n",
      "Epoch 11/50\n",
      "899820/899820 [==============================] - 7s 8us/step - loss: 1.3299e-04 - val_loss: 2.2189e-04\n",
      "Epoch 12/50\n",
      "899820/899820 [==============================] - 7s 7us/step - loss: 1.3112e-04 - val_loss: 2.2469e-04\n",
      "Epoch 13/50\n",
      "899820/899820 [==============================] - 7s 8us/step - loss: 1.2941e-04 - val_loss: 2.2257e-04\n",
      "Epoch 14/50\n",
      "899820/899820 [==============================] - 7s 8us/step - loss: 1.2786e-04 - val_loss: 2.2429e-04\n",
      "Epoch 15/50\n",
      "899820/899820 [==============================] - 7s 8us/step - loss: 1.2643e-04 - val_loss: 2.1812e-04\n",
      "Epoch 16/50\n",
      "899820/899820 [==============================] - 8s 8us/step - loss: 1.2501e-04 - val_loss: 2.1330e-04\n",
      "Epoch 17/50\n",
      "899820/899820 [==============================] - 7s 8us/step - loss: 1.2371e-04 - val_loss: 2.1910e-04\n",
      "Epoch 18/50\n",
      "899820/899820 [==============================] - 7s 8us/step - loss: 1.2238e-04 - val_loss: 2.1304e-04\n",
      "Epoch 19/50\n",
      "899820/899820 [==============================] - 7s 8us/step - loss: 1.2070e-04 - val_loss: 2.1381e-04\n",
      "Epoch 20/50\n",
      "899820/899820 [==============================] - 7s 8us/step - loss: 1.1826e-04 - val_loss: 2.1039e-04\n",
      "Epoch 21/50\n",
      "899820/899820 [==============================] - 7s 8us/step - loss: 1.1465e-04 - val_loss: 2.0548e-04\n",
      "Epoch 22/50\n",
      "899820/899820 [==============================] - 8s 9us/step - loss: 1.1072e-04 - val_loss: 2.0537e-04\n",
      "Epoch 23/50\n",
      "899820/899820 [==============================] - 7s 8us/step - loss: 1.0796e-04 - val_loss: 1.9851e-04\n",
      "Epoch 24/50\n",
      "899820/899820 [==============================] - 8s 9us/step - loss: 1.0606e-04 - val_loss: 1.9636e-04\n",
      "Epoch 25/50\n",
      "899820/899820 [==============================] - 7s 8us/step - loss: 1.0479e-04 - val_loss: 2.0018e-04\n",
      "Epoch 26/50\n",
      "899820/899820 [==============================] - 7s 8us/step - loss: 1.0385e-04 - val_loss: 2.0023e-04\n",
      "Epoch 27/50\n",
      "899820/899820 [==============================] - 7s 7us/step - loss: 1.0327e-04 - val_loss: 1.9711e-04\n",
      "Epoch 28/50\n",
      "899820/899820 [==============================] - 7s 8us/step - loss: 1.0275e-04 - val_loss: 1.9404e-04\n",
      "Epoch 29/50\n",
      "899820/899820 [==============================] - 7s 8us/step - loss: 1.0242e-04 - val_loss: 1.9667e-04\n",
      "Epoch 30/50\n",
      "899820/899820 [==============================] - 8s 9us/step - loss: 1.0215e-04 - val_loss: 1.9768e-04\n",
      "Epoch 31/50\n",
      "899820/899820 [==============================] - 8s 9us/step - loss: 1.0191e-04 - val_loss: 1.9082e-04\n",
      "Epoch 32/50\n",
      "899820/899820 [==============================] - 8s 9us/step - loss: 1.0167e-04 - val_loss: 1.9965e-04\n",
      "Epoch 33/50\n",
      "899820/899820 [==============================] - 7s 7us/step - loss: 1.0153e-04 - val_loss: 1.9369e-04\n",
      "Epoch 34/50\n",
      "899820/899820 [==============================] - 7s 8us/step - loss: 1.0135e-04 - val_loss: 1.8979e-04\n",
      "Epoch 35/50\n",
      "899820/899820 [==============================] - 7s 7us/step - loss: 1.0120e-04 - val_loss: 1.8892e-04\n",
      "Epoch 36/50\n",
      "899820/899820 [==============================] - 7s 8us/step - loss: 1.0105e-04 - val_loss: 1.9268e-04\n",
      "Epoch 37/50\n",
      "899820/899820 [==============================] - 7s 7us/step - loss: 1.0094e-04 - val_loss: 1.9700e-04\n",
      "Epoch 38/50\n",
      "899820/899820 [==============================] - 8s 8us/step - loss: 1.0078e-04 - val_loss: 1.9014e-04\n",
      "Epoch 39/50\n",
      "899820/899820 [==============================] - 7s 8us/step - loss: 1.0061e-04 - val_loss: 1.8881e-04\n",
      "Epoch 40/50\n",
      "899820/899820 [==============================] - 8s 9us/step - loss: 1.0051e-04 - val_loss: 1.9891e-04\n",
      "Epoch 41/50\n",
      "899820/899820 [==============================] - 7s 8us/step - loss: 1.0029e-04 - val_loss: 1.9669e-04\n",
      "Epoch 42/50\n",
      "899820/899820 [==============================] - 7s 8us/step - loss: 1.0007e-04 - val_loss: 1.9225e-04\n",
      "Epoch 43/50\n",
      "899820/899820 [==============================] - 8s 9us/step - loss: 9.9914e-05 - val_loss: 1.9642e-04\n",
      "Epoch 44/50\n",
      "899820/899820 [==============================] - 7s 8us/step - loss: 9.9641e-05 - val_loss: 1.8898e-04\n",
      "Epoch 45/50\n",
      "899820/899820 [==============================] - 8s 9us/step - loss: 9.9481e-05 - val_loss: 1.8977e-04\n",
      "Epoch 46/50\n",
      "899820/899820 [==============================] - 6s 7us/step - loss: 9.9274e-05 - val_loss: 1.9618e-04\n",
      "Epoch 47/50\n",
      "899820/899820 [==============================] - 6s 7us/step - loss: 9.9049e-05 - val_loss: 1.9116e-04\n",
      "Epoch 48/50\n",
      "899820/899820 [==============================] - 7s 8us/step - loss: 9.8895e-05 - val_loss: 1.8956e-04\n",
      "Epoch 49/50\n",
      "899820/899820 [==============================] - 7s 8us/step - loss: 9.8705e-05 - val_loss: 1.8807e-04\n",
      "Epoch 50/50\n",
      "899820/899820 [==============================] - 8s 9us/step - loss: 9.8581e-05 - val_loss: 1.9807e-04\n"
     ]
    }
   ],
   "source": [
    "# Train model for 100 epochs, batch size of 10: \n",
    "NUM_EPOCHS=50\n",
    "BATCH_SIZE=1028\n",
    "\n",
    "history=model.fit(X_train, X_train,\n",
    "                  batch_size=BATCH_SIZE, \n",
    "                  epochs=NUM_EPOCHS,\n",
    "                  validation_split=0.1,\n",
    "                  verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pred = model.predict(X_train)\n",
    "scored = pd.DataFrame()\n",
    "scored['Loss_mse'] = np.mean(np.abs(X_pred-X_train), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pred_test = model.predict(X_test)\n",
    "scored_test = pd.DataFrame()\n",
    "scored_test['Loss_mse'] = np.mean(np.abs(X_pred_test-X_test), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Time (s)')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(20,2))\n",
    "plt.plot(train_time, scored['Loss_mse'])\n",
    "plt.ylim([0, 30000])\n",
    "plt.xlabel(\"Time (s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Time (s)')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(20,2))\n",
    "plt.plot(test_time, scored_test['Loss_mse'])\n",
    "plt.ylim([0, 30000])\n",
    "#plt.xlim([-20,0])\n",
    "plt.xlabel(\"Time (s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learn on other ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flushing input history\n",
      "Flushing input history\n",
      "Flushing input history\n",
      "Flushing input history\n",
      "Flushing input history\n",
      "Flushing input history\n",
      "Flushing input history\n",
      "Flushing input history\n",
      "Flushing input history\n",
      "Flushing input history\n",
      "Flushing input history\n",
      "Flushing input history\n",
      "Flushing input history\n",
      "Flushing input history\n",
      "Flushing input history\n",
      "Flushing input history\n",
      "Flushing input history\n",
      "Flushing input history\n",
      "Flushing input history\n",
      "Flushing input history\n",
      "Flushing input history\n",
      "Flushing input history\n",
      "Flushing input history\n",
      "Flushing input history\n",
      "Flushing input history\n",
      "Flushing input history\n",
      "Flushing input history\n",
      "Flushing input history\n",
      "X_train's shape:  (999800, 12)\n",
      "X_test's shape:  (999800, 12)\n"
     ]
    }
   ],
   "source": [
    "X_train, train_time = generate_data(\"./data/Ramp19/\", time_range = (-200, -100), window = 2000, step = 10)\n",
    "X_test, test_time = generate_data(\"./data/Ramp19/\", time_range = (-100, 0), window = 2000, step = 10)\n",
    "print(\"X_train's shape: \", X_train.shape)\n",
    "print(\"X_test's shape: \", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train = scaler.transform(X_train)\n",
    "\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#================BUILD THE MODEL====================\n",
    "print(\"Using Keras version: \", keras.__version__)\n",
    "\n",
    "# Simple model\n",
    "model = models.Sequential()\n",
    "\n",
    "model.add(Dense(6, activation = 'elu', kernel_initializer='glorot_uniform',\n",
    "                kernel_regularizer=regularizers.l2(0.0),\n",
    "                input_dim=X_train.shape[1]))\n",
    "\n",
    "model.add(Dense(3,activation='elu',\n",
    "                kernel_initializer='glorot_uniform'))\n",
    "\n",
    "model.add(Dense(6,activation='elu',\n",
    "                kernel_initializer='glorot_uniform'))\n",
    "\n",
    "model.add(Dense(X_train.shape[1],\n",
    "                kernel_initializer='glorot_uniform'))\n",
    "\n",
    "model.compile(loss='mse',optimizer='adam')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model for 100 epochs, batch size of 10: \n",
    "NUM_EPOCHS=50\n",
    "BATCH_SIZE=1028\n",
    "\n",
    "history=model.fit(X_train, X_train,\n",
    "                  batch_size=BATCH_SIZE, \n",
    "                  epochs=NUM_EPOCHS,\n",
    "                  validation_split=0.1,\n",
    "                  verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pred = model.predict(X_train)\n",
    "scored = pd.DataFrame()\n",
    "scored['Loss_mse'] = np.mean(np.abs(X_pred-X_train), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pred_test = model.predict(X_test)\n",
    "scored_test = pd.DataFrame()\n",
    "scored_test['Loss_mse'] = np.mean(np.abs(X_pred_test-X_test), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,2))\n",
    "plt.plot(train_time, scored['Loss_mse'])\n",
    "#plt.ylim([0, 30])\n",
    "plt.xlabel(\"Time (s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,2))\n",
    "plt.plot(test_time, scored_test['Loss_mse'])\n",
    "#plt.ylim([0, 30])\n",
    "#plt.xlim([-20,0])\n",
    "plt.xlabel(\"Time (s)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
